{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "from PIL import Image\n",
    "from pickle import dump, load\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and preprocess images\n",
    "def load_images(images_folder, save_file_to=None):\n",
    "    X_original = []\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for subdir, dirs, files in os.walk(images_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(('jpg', 'jpeg', 'png')):\n",
    "                img_path = os.path.join(subdir, file)\n",
    "                label = os.path.basename(subdir)\n",
    "\n",
    "                image = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "                image = image.resize((48, 48))  # Resize to 48x48\n",
    "                X_original.append(np.array(image).flatten())\n",
    "                X.append(np.array(image).flatten())\n",
    "                y.append(label)\n",
    "\n",
    "    if save_file_to:\n",
    "        with open(save_file_to, \"wb\") as f:\n",
    "            dump((X_original, X, y), f, protocol=5)\n",
    "\n",
    "    return np.array(X_original), np.array(X), np.array(y)\n",
    "\n",
    "images_folder = \"../images\"\n",
    "dataset_file = \"dataset_dump.pkl\"\n",
    "\n",
    "\n",
    "X_original, X, y = load_images(images_folder, save_file_to=dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset_dump.pkl\", \"rb\") as f:\n",
    "    X_original,X,y = load(f)\n",
    "    \n",
    "X\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data function\n",
    "def preprocessing_data(X, y, save_file_to=None):\n",
    "    # Normalize pixel values\n",
    "    X = X / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Apply PCA for dimensionality reduction\n",
    "    pca = PCA(n_components=100)  # Adjust the number of components as needed\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "    if save_file_to:\n",
    "        with open(save_file_to, \"wb\") as f:\n",
    "            dump((X_pca, y_encoded, label_encoder, scaler), f, protocol=5)\n",
    "\n",
    "    return X_pca, y_encoded, label_encoder, scaler\n",
    "\n",
    "# Process the data and save\n",
    "X_scaled, y_encoded, label_encoder, scaler = preprocessing_data(X, y, save_file_to=\"preprocessed_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"preprocessed_data.pkl\", \"rb\") as f:\n",
    "    X_scaled, y_encoded, label_encoder, pca, scaler = load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y_encoded, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset to handle PCA data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Reshape the PCA data as an image for visualization purposes\n",
    "        image = self.X[idx].reshape(10, 10)  # Example reshape (adjust based on PCA output)\n",
    "        \n",
    "        # Apply the transformation if defined\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = self.y[idx]\n",
    "        # Ensure the image is of type float32\n",
    "        return image.type(torch.float32), label  # This line is added\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(10),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = CustomDataset(X_train, y_train, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)  # Increased neurons\n",
    "        self.fc2 = nn.Linear(512, 256)        # Added extra layer\n",
    "        self.fc3 = nn.Linear(256, len(np.unique(y_encoded)))  # Output layer\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))  # Apply relu after the second layer\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleNN(input_size=100)  # Adjust based on PCA output size\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Add learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images.view(images.size(0), -1))  # Flatten images for FC layer\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation and prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    y_pred = predicted.numpy()\n",
    "\n",
    "# Calculate accuracy and print classification report\n",
    "accuracy = accuracy_score(y_test_tensor.numpy(), y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_tensor.numpy(), y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "def visualize_predictions(model, X_test, y_test, label_encoder, X_original):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        _, y_pred = torch.max(outputs.data, 1)\n",
    "\n",
    "    random_indices = np.random.randint(0, len(y_test), 5)\n",
    "\n",
    "    for idx in random_indices:\n",
    "        original_image = X_original[idx].reshape(48, 48)  # Reshape original image for visualization\n",
    "        plt.imshow(original_image, cmap='gray')\n",
    "        plt.title(f\"True: {label_encoder.inverse_transform([y_test[idx]])[0]}, \"\n",
    "                  f\"Predicted: {label_encoder.inverse_transform([y_pred[idx]])[0]}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Call visualization function\n",
    "visualize_predictions(model, X_test_tensor, y_test_tensor, label_encoder, X_original)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
