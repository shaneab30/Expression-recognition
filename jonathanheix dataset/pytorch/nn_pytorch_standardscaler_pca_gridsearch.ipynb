{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "from PIL import Image\n",
    "from pickle import dump, load\n",
    "from sklearn.svm import SVC\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(images_folder, save_file_to=None):\n",
    "    X_original = []\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(images_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(('jpg', 'jpeg', 'png')):\n",
    "                img_path = os.path.join(subdir, file)\n",
    "                label = os.path.basename(subdir)\n",
    "                \n",
    "                image = Image.open(img_path).convert('L')\n",
    "                image = image.resize((48, 48))\n",
    "                X_original.append(np.array(image).flatten())\n",
    "                X.append(np.array(image).flatten())\n",
    "                y.append(label)\n",
    "                \n",
    "    if save_file_to:\n",
    "        with open(save_file_to, \"wb\") as f:\n",
    "            dump((X_original, X, y), f, protocol=5)\n",
    "                \n",
    "    return np.array(X_original), np.array(X), np.array(y)\n",
    "\n",
    "# Load images and save the dataset for reuse\n",
    "images_folder = '../images'\n",
    "dataset_file = \"D:/projects/machine learning/Expression-recognition/jonathanheix dataset/final/dataset_dump.pkl\"\n",
    "\n",
    "X_original, X, y = load_images(images_folder, save_file_to=dataset_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset_dump.pkl\", \"rb\") as f:\n",
    "    X_original, X, y = load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_data(X, y, save_file_to=None):\n",
    "    # Normalize pixel values\n",
    "    X = X / 255.0  # Normalize to [0, 1]\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Dimensionality reduction with PCA\n",
    "    pca = PCA(n_components=100)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_reduced)\n",
    "    \n",
    "    if save_file_to:\n",
    "        with open(save_file_to, \"wb\") as f:\n",
    "            dump((X_scaled, y_encoded, label_encoder, pca, scaler), f, protocol=5)\n",
    "    \n",
    "    return X_scaled, y_encoded, label_encoder, pca, scaler\n",
    "\n",
    "X_scaled, y_encoded, label_encoder, pca, scaler = preprocessing_data(X, y, save_file_to=\"labelencoder_standardscaler_pca_normalizers_dump.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(100, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, len(np.unique(y_encoded)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply ReLU activation after each layer\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout after first layer\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))  # Apply ReLU after third layer\n",
    "        x = self.fc4(x)  # Output layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'num_epochs': [500, 1000],  # Example: try 500 and 1000 epochs\n",
    "    'learning_rate': [0.001, 0.0005],  # Example: try different learning rates\n",
    "    'batch_size': [32, 64],  # Example: try different batch sizes\n",
    "}\n",
    "\n",
    "# Grid search to try each combination of parameters\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "# Loop through each combination of parameters\n",
    "for params in grid:\n",
    "    print(f\"Training with parameters: {params}\")\n",
    "    \n",
    "    # Extract parameters\n",
    "    num_epochs = params['num_epochs']\n",
    "    learning_rate = params['learning_rate']\n",
    "    batch_size = params['batch_size']\n",
    "\n",
    "    # Redefine the model, optimizer, and criterion for each combination\n",
    "    model = SimpleNN()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model with the current combination of hyperparameters\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Split data into batches\n",
    "        for i in range(0, len(X_train_tensor), batch_size):\n",
    "            X_batch = X_train_tensor[i:i+batch_size]\n",
    "            y_batch = y_train_tensor[i:i+batch_size]\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:  # Print loss every 100 epochs\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Evaluate the model after training\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "        print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "        # Print classification report\n",
    "        report = classification_report(y_test_tensor, predicted, target_names=label_encoder.classes_)\n",
    "        print(f\"Classification Report:\\n{report}\")\n",
    "\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "def visualize_predictions(model, X_test, y_test, label_encoder, X_original):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        _, y_pred = torch.max(outputs.data, 1)\n",
    "    \n",
    "    random_indices = np.random.randint(0, len(y_test), 5)\n",
    "    \n",
    "    for idx in random_indices:\n",
    "        original_image = X_original[idx].reshape(48, 48)  # Reshape to 48x48\n",
    "        plt.imshow(original_image, cmap='gray')\n",
    "        plt.title(f\"True: {label_encoder.inverse_transform([y_test[idx]])[0]}, \"\n",
    "                  f\"Predicted: {label_encoder.inverse_transform([y_pred[idx]])[0]}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "visualize_predictions(model, X_test_tensor, y_test_tensor, label_encoder, X_original)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
